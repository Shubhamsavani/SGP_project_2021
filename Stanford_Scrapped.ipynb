{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping the Website for its Courses and information regarding the courses\n",
    "\n",
    "\n",
    "## Intro to Web Scrapping\n",
    "- Web scraping is a technique to fetch data from websites. \n",
    "- One way is to manually copy-paste the data, which both tedious and time-consuming. Web Scraping is the automation of the data extraction process from websites.\n",
    "\n",
    "## Tools I had used\n",
    "- Python\n",
    "    - requests library\n",
    "    - BeautifulSoup \n",
    "    - Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am going to scrape 'https://online.stanford.edu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline  (Step by step How we had written the code)\n",
    "- I am going to use requests library to download the webpages\n",
    "- Then i am going to decide the info i want to scrape \n",
    "- I will use BeautifulSoup to parse the HTML code of page\n",
    "- Then i am indentifying the tags of info i wanna scrap\n",
    "- I am storing it in a list then storing them into dictionary and then i am creating a data frame using pandas \n",
    "- Then I will save into csv file which will look like this format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating various functions each performing unique task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This Below function will use get() method from requests library and download the webpages.\n",
    "- Then it will store the text of response we got from the get()\n",
    "- We will parse that text using BeautifulSoup for getting information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_url(URL):\n",
    "    \n",
    "#  Following code for Bypassing the url because it contains robots\n",
    "    Headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'\n",
    "    }\n",
    "\n",
    "# Here we are using  get() method in requests library,it will return a response object so we will create a variable to store that \n",
    "# response\n",
    "    response=requests.get(URL, headers=Headers)\n",
    "    content = response.text\n",
    "\n",
    "# If the status code is 200 then it means that webpage is downloaded succesfully\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to Load Page\")\n",
    "\n",
    "# Parsing the HTML code using BeautifulSoup\n",
    "    doc = BeautifulSoup(content,'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This below function will search the tags which contains the Topic names\n",
    "    - We will search for tags by right clicking on the topic and then go to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_tags(doc):\n",
    "    h3_tags = doc.find_all('h3')\n",
    "    return h3_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After searching tags from the HTMl code of webpage we will get the text of that tag,and for getting the every name from every tag in a webpage , We will use loop\n",
    "    - So I have taken an empty list, in which i am going to store the Topic names\n",
    "\n",
    "    - In loop i am traversing through each tag and storing text of each tag in list\n",
    "    \n",
    "    - Then, returning the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(h3_tags):\n",
    "    topics_list = []\n",
    "    for tag in h3_tags:\n",
    "        topics_list.append(tag.text)\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now In the below method I am going to find the tags whose text contains the url of each course\n",
    "    - At first, if we look at the parameter list , i have passed the doc , the HTML code of whole website which we had parsed and the topic_tags \n",
    "    list in which we had stored the tags which contains the topic names.\n",
    "   \n",
    "    - Then by right clicking on url and then going to inspect I had observed that the tags containing url is parent of parent of tag containing \n",
    "    topic names.\n",
    "   \n",
    "    - So I had taken an empty list to store the tags containing url \n",
    "    \n",
    "    - Then, I am traversing through the list containg topic tags and extracting the tags which contains url\n",
    "    \n",
    "    - Then, returning the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_tags(doc,topic_tags):\n",
    "    a_tags = [] # for storing the tags which contains url\n",
    "    for tag in topic_tags:\n",
    "        a_tags.append(tag.parent.parent)\n",
    "    return a_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the below function we will extract the urls from the tags which we have collected\n",
    "    - First getting an empty list for storing the urls of respective courses\n",
    "\n",
    "    - Creating base url , which is common in every url of webpage\n",
    "\n",
    "    - Then traversing through every tags , then searching text(url) in tag through attribute 'href' then storing url after concating behind \n",
    "    base url\n",
    "    \n",
    "    - Then, returning list containing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url_tags):\n",
    "    urls_list = []\n",
    "    base_url = 'https://online.stanford.edu'\n",
    "    for tag in url_tags:\n",
    "        urls_list.append(base_url + tag['href'])\n",
    "    return urls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the info regarding the name of school is inside the urls we had scrapped ,So now we will scrape every urls\n",
    "    - First created an empty list for storing the school names\n",
    "    \n",
    "    - Then traversing through the url list\n",
    "\n",
    "    - Now by right clicking and then going to inspect , by observing we get to know that name of school is in p tag\n",
    "\n",
    "    - So if p tag and specific class with that tag is present in the url then we will call parsing_url(url) method which will parse url \n",
    "    passed inside the parameter\n",
    "\n",
    "    - Then we are going to collect the first occourence of p tag \n",
    "        - Then searching for the first a tag because it contains the schoolname (Info we want)\n",
    "\n",
    "        -After finding the tag in which school name is there, we will then append (text of that tag)info  in empty list \n",
    "    \n",
    "    - Then, returning the list containing school names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_school_names(List_Of_Urls):\n",
    "    schools = []\n",
    "    for i in range(len(List_Of_Urls)): \n",
    "# Similarly we will parse each url to get the info about school name which is present in urls, So we will scrape that info  \n",
    "        page_doc = parsing_url(List_Of_Urls[i])\n",
    "# If page contains p tag and that specific class then it will return true else false\n",
    "        if(page_doc.find('p',class_= 'text-red text-strong')):\n",
    "    # Then I am collecting the first occourence of p tag \n",
    "            p_tags = page_doc.find('p',class_= 'text-red text-strong')\n",
    "    # Then searching for the first a tag because it contains the schoolname (Info we want)\n",
    "            a_tags = p_tags.find('a')\n",
    "        # After finding the tag in which school name is there, we will then store (text of that tag)info  in list \n",
    "            schools.append(a_tags.text)\n",
    "        else:\n",
    "            schools.append(None)\n",
    "    return schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the ways for creating DataFrame is by using pandas \n",
    "\n",
    "    - Creating dictionary and giving each coloumn a name , and what info must be there in each coloumn, that list name\n",
    "\n",
    "    - Then , returning data frane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converting_into_Dataframe():\n",
    "#  Creating dataframe using pandas , by creating a dictionary\n",
    "    dict_scrapped_things = {\n",
    "        'Topic_Titles' : Topic_list,\n",
    "        'School_Name' : school_name,\n",
    "        'Link' : url_list\n",
    "    }\n",
    "    return pd.DataFrame(dict_scrapped_things)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following function , I am just calling all the function which we had defined above \n",
    "\n",
    "    - But i am traversing through 8 different webpages and performing these following tasks on each webpage\n",
    "    \n",
    "        - Parsing url\n",
    "\n",
    "        - Then collecting topic tags, url tags\n",
    "        \n",
    "        - From topic tags , collecting topic names , from url tags collecting urls , and from urls collecting school names\n",
    "\n",
    "        - Then converting into data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_list = []\n",
    "url_list = []\n",
    "school_name = []\n",
    "\n",
    "dummy_url='https://online.stanford.edu/search-catalog?type=All&topics[1054]=1054&topics[1049]=1049&topics[1066]=1066&topics[1069]=1069&topics[1070]=1070&topics[1059]=1059&topics[1047]=1047&topics[1057]=1057&topics[1064]=1064&topics[1073]=1073&topics[1062]=1062&topics[1060]=1060&topics[1065]=1065&topics[1063]=1063&topics[1061]=1061&topics[1094]=1094&topics[1043]=1043&topics[1050]=1050&topics[1048]=1048&topics[1072]=1072&topics[1045]=1045&topics[1042]=1042&topics[1046]=1046&topics[1044]=1044&topics[1055]=1055&topics[1071]=1071&topics[1053]=1053&topics[1052]=1052&topics[1068]=1068&topics[1067]=1067&topics[1098]=1098&topics[1058]=1058&topics[1079]=1079&topics[1051]=1051&topics[1056]=1056&free_or_paid[free]=free&page={k:d}'\n",
    "\n",
    "######################################## LOOP BEGINS HERE #################################################################    \n",
    "\n",
    "for i in range(8): # Terminating condition is till 8 because there are 8 webpages of courses We had scraped\n",
    "    url = dummy_url.format(k=i)  #There is only one difference in url ,is the number at the end, so I am using format() method\n",
    "\n",
    "# doc is parsed HTML code\n",
    "    doc = parsing_url(url)\n",
    "\n",
    "#  Collecting tags for Topic titles and then through tags extracting the names of Topics\n",
    "    Topic_tags = extract_topic_tags(doc)\n",
    "    Topic_list += (get_topics(Topic_tags))\n",
    "\n",
    "# Collecting tags for Topic_urls and then through tags extracting links for each topic\n",
    "    url_tags = extract_url_tags(doc,Topic_tags)\n",
    "    url_list += (get_urls(url_tags)) \n",
    "\n",
    "######################################## LOOP ENDS HERE #################################################################    \n",
    "\n",
    "# Collecting the names of school of each course\n",
    "school_name = get_school_names(url_list)\n",
    "\n",
    "# Converting the collected info into data frame using pandas\n",
    "Topics_Dataframe = converting_into_Dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: If csv file of one name is created you can't run code to create csv file of same name It will deny your request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our dataframe to csv file using to_csv('name.csv') method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data frame we created into csv file\n",
    "\n",
    "Topics_Dataframe.to_csv('Coursess_Stanford.csv',index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1084118ec0a7608ab883fcf903db7d8ac769b926a9010f87416086cf45542a46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
